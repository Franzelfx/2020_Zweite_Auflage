{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Ein Einfaches Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash:  33918\n"
     ]
    }
   ],
   "source": [
    "# Generate random hash\n",
    "hash = random.getrandbits(16)\n",
    "print(\"Hash: \", hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade den Iris-Datenset\n",
    "data_train = pd.read_csv('./input/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die 3 zu erkennenden Klassifikationsklassen werden zu numerischen Werten 0, 1 bzw. 2 umgewandelt.\n",
    "data_train.loc[data_train['species']=='Iris-setosa', 'species']=0\n",
    "data_train.loc[data_train['species']=='Iris-versicolor', 'species']=1\n",
    "data_train.loc[data_train['species']=='Iris-virginica', 'species']=2\n",
    "data_train = data_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der eingelesene Datenset wird als Matrix dargestellt\n",
    "data_train_array = data_train.values # oder data_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zur Sicherstellung der Reproduzierbarkeit der Ergebnisse setzen wir random.seed auf eine festen Wert, z.B. 42\n",
    "np.random.seed(17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Datenset wird in zwei separate Kategorie gespaltet: Testdaten und Trainingsdaten. \n",
    "\n",
    "80% der Daten werden zum Trainieren und 20% zum Testen des Modells verwendet. \n",
    "\n",
    "Da es sich bei der Eingabe um einen Vektor handelt, werden wird den Großbuchstaben X benutzen.\n",
    "\n",
    "Für die Ausgabe hingegen handelt es sich um ein einzelner Werte, \n",
    "daher die Bezeichung mit dem Kleinbuchstaben y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train_array[:,:4],\n",
    "                                                    data_train_array[:,4],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "Ein neuronales Netz zur Klassifikation (MultiLayerPerceptron) wird mit folgenden Eigenschaften gebildet:\n",
    "- einem Input-Layer mit 4 Neuronen, die die Merkmale der Iris-Planze repräsentieren\n",
    "- einem Hidden-Layer mit 10 Neuronen\n",
    "- einem Output-Layer mit 3 Neuronen, die die zu erkennenden Klassen repräsentieren\n",
    "\n",
    "Dabei wird als Aktivierungsfunktion relu und als Optimierer adam verwenden.\n",
    "\n",
    "<img src=\"img/network_01.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp = MLPClassifier(hidden_layer_sizes=(10,),activation='relu', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2\n",
    "Erstelle eine zweite Version des neuronalen Netzes mit folgenden Eigenschaften:\n",
    "- einem Input-Layer mit 4 Neuronen, die die Merkmale der Iris-Planze repräsentieren\n",
    "- zwei Hidden-Layer mit jeweils 3 und 5 Neuronen\n",
    "- einem Output-Layer mit 3 Neuronen, die die zu erkennenden Klassen repräsentieren\n",
    "\n",
    "Füge die Zweite Version direkt unter der Ersten ein. Und führe die neu Zelle und alle folgenden aus.\n",
    "\n",
    "<img src=\"img/network_02.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zweite Version:\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(5,5),activation='relu', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.61322024\n",
      "Iteration 2, loss = 3.96999130\n",
      "Iteration 3, loss = 3.46609256\n",
      "Iteration 4, loss = 3.05074659\n",
      "Iteration 5, loss = 2.68715213\n",
      "Iteration 6, loss = 2.36526872\n",
      "Iteration 7, loss = 2.10490058\n",
      "Iteration 8, loss = 1.88388378\n",
      "Iteration 9, loss = 1.70005718\n",
      "Iteration 10, loss = 1.53747357\n",
      "Iteration 11, loss = 1.38634665\n",
      "Iteration 12, loss = 1.26373454\n",
      "Iteration 13, loss = 1.15350769\n",
      "Iteration 14, loss = 1.05534227\n",
      "Iteration 15, loss = 0.96766113\n",
      "Iteration 16, loss = 0.90694030\n",
      "Iteration 17, loss = 0.86937831\n",
      "Iteration 18, loss = 0.84440678\n",
      "Iteration 19, loss = 0.82266434\n",
      "Iteration 20, loss = 0.80347524\n",
      "Iteration 21, loss = 0.78536554\n",
      "Iteration 22, loss = 0.77001276\n",
      "Iteration 23, loss = 0.75557412\n",
      "Iteration 24, loss = 0.74055971\n",
      "Iteration 25, loss = 0.72887897\n",
      "Iteration 26, loss = 0.71636352\n",
      "Iteration 27, loss = 0.70480566\n",
      "Iteration 28, loss = 0.69453821\n",
      "Iteration 29, loss = 0.68443857\n",
      "Iteration 30, loss = 0.67505473\n",
      "Iteration 31, loss = 0.66582438\n",
      "Iteration 32, loss = 0.65733333\n",
      "Iteration 33, loss = 0.64934201\n",
      "Iteration 34, loss = 0.64122674\n",
      "Iteration 35, loss = 0.63371689\n",
      "Iteration 36, loss = 0.62648455\n",
      "Iteration 37, loss = 0.61940850\n",
      "Iteration 38, loss = 0.61271281\n",
      "Iteration 39, loss = 0.60613983\n",
      "Iteration 40, loss = 0.60004151\n",
      "Iteration 41, loss = 0.59424994\n",
      "Iteration 42, loss = 0.58799354\n",
      "Iteration 43, loss = 0.58250797\n",
      "Iteration 44, loss = 0.57786745\n",
      "Iteration 45, loss = 0.57177994\n",
      "Iteration 46, loss = 0.56686422\n",
      "Iteration 47, loss = 0.56217721\n",
      "Iteration 48, loss = 0.55762316\n",
      "Iteration 49, loss = 0.55273070\n",
      "Iteration 50, loss = 0.54835425\n",
      "Iteration 51, loss = 0.54436492\n",
      "Iteration 52, loss = 0.53984176\n",
      "Iteration 53, loss = 0.53581086\n",
      "Iteration 54, loss = 0.53207704\n",
      "Iteration 55, loss = 0.52830460\n",
      "Iteration 56, loss = 0.52493430\n",
      "Iteration 57, loss = 0.52111711\n",
      "Iteration 58, loss = 0.51779444\n",
      "Iteration 59, loss = 0.51456407\n",
      "Iteration 60, loss = 0.51184120\n",
      "Iteration 61, loss = 0.50851471\n",
      "Iteration 62, loss = 0.50564646\n",
      "Iteration 63, loss = 0.50298094\n",
      "Iteration 64, loss = 0.50011071\n",
      "Iteration 65, loss = 0.49763185\n",
      "Iteration 66, loss = 0.49526369\n",
      "Iteration 67, loss = 0.49291161\n",
      "Iteration 68, loss = 0.49054277\n",
      "Iteration 69, loss = 0.48847226\n",
      "Iteration 70, loss = 0.48633815\n",
      "Iteration 71, loss = 0.48413457\n",
      "Iteration 72, loss = 0.48215626\n",
      "Iteration 73, loss = 0.48033878\n",
      "Iteration 74, loss = 0.47849330\n",
      "Iteration 75, loss = 0.47663984\n",
      "Iteration 76, loss = 0.47514128\n",
      "Iteration 77, loss = 0.47330760\n",
      "Iteration 78, loss = 0.47192157\n",
      "Iteration 79, loss = 0.47017766\n",
      "Iteration 80, loss = 0.46869775\n",
      "Iteration 81, loss = 0.46751247\n",
      "Iteration 82, loss = 0.46578113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 83, loss = 0.46445463\n",
      "Iteration 84, loss = 0.46324425\n",
      "Iteration 85, loss = 0.46196541\n",
      "Iteration 86, loss = 0.46064934\n",
      "Iteration 87, loss = 0.45939069\n",
      "Iteration 88, loss = 0.45845579\n",
      "Iteration 89, loss = 0.45714615\n",
      "Iteration 90, loss = 0.45606127\n",
      "Iteration 91, loss = 0.45543284\n",
      "Iteration 92, loss = 0.45430488\n",
      "Iteration 93, loss = 0.45285273\n",
      "Iteration 94, loss = 0.45198646\n",
      "Iteration 95, loss = 0.45091480\n",
      "Iteration 96, loss = 0.44993477\n",
      "Iteration 97, loss = 0.44913352\n",
      "Iteration 98, loss = 0.44817431\n",
      "Iteration 99, loss = 0.44723040\n",
      "Iteration 100, loss = 0.44648137\n",
      "Iteration 101, loss = 0.44550993\n",
      "Iteration 102, loss = 0.44491669\n",
      "Iteration 103, loss = 0.44429945\n",
      "Iteration 104, loss = 0.44309980\n",
      "Iteration 105, loss = 0.44251133\n",
      "Iteration 106, loss = 0.44214360\n",
      "Iteration 107, loss = 0.44097561\n",
      "Iteration 108, loss = 0.44028579\n",
      "Iteration 109, loss = 0.43924550\n",
      "Iteration 110, loss = 0.43849055\n",
      "Iteration 111, loss = 0.43780715\n",
      "Iteration 112, loss = 0.43695813\n",
      "Iteration 113, loss = 0.43634017\n",
      "Iteration 114, loss = 0.43581604\n",
      "Iteration 115, loss = 0.43502114\n",
      "Iteration 116, loss = 0.43423124\n",
      "Iteration 117, loss = 0.43380800\n",
      "Iteration 118, loss = 0.43306230\n",
      "Iteration 119, loss = 0.43219221\n",
      "Iteration 120, loss = 0.43161995\n",
      "Iteration 121, loss = 0.43082223\n",
      "Iteration 122, loss = 0.43027381\n",
      "Iteration 123, loss = 0.42964748\n",
      "Iteration 124, loss = 0.42891334\n",
      "Iteration 125, loss = 0.42841163\n",
      "Iteration 126, loss = 0.42774052\n",
      "Iteration 127, loss = 0.42710288\n",
      "Iteration 128, loss = 0.42646820\n",
      "Iteration 129, loss = 0.42602753\n",
      "Iteration 130, loss = 0.42535777\n",
      "Iteration 131, loss = 0.42530197\n",
      "Iteration 132, loss = 0.42489374\n",
      "Iteration 133, loss = 0.42357552\n",
      "Iteration 134, loss = 0.42309118\n",
      "Iteration 135, loss = 0.42231733\n",
      "Iteration 136, loss = 0.42205250\n",
      "Iteration 137, loss = 0.42118945\n",
      "Iteration 138, loss = 0.42065355\n",
      "Iteration 139, loss = 0.42032970\n",
      "Iteration 140, loss = 0.41941833\n",
      "Iteration 141, loss = 0.41897560\n",
      "Iteration 142, loss = 0.41870072\n",
      "Iteration 143, loss = 0.41802292\n",
      "Iteration 144, loss = 0.41734924\n",
      "Iteration 145, loss = 0.41664952\n",
      "Iteration 146, loss = 0.41634423\n",
      "Iteration 147, loss = 0.41563354\n",
      "Iteration 148, loss = 0.41504346\n",
      "Iteration 149, loss = 0.41455574\n",
      "Iteration 150, loss = 0.41383866\n",
      "Iteration 151, loss = 0.41355627\n",
      "Iteration 152, loss = 0.41269301\n",
      "Iteration 153, loss = 0.41312587\n",
      "Iteration 154, loss = 0.41145159\n",
      "Iteration 155, loss = 0.41100966\n",
      "Iteration 156, loss = 0.41044416\n",
      "Iteration 157, loss = 0.40984317\n",
      "Iteration 158, loss = 0.40952172\n",
      "Iteration 159, loss = 0.40875550\n",
      "Iteration 160, loss = 0.40837108\n",
      "Iteration 161, loss = 0.40767794\n",
      "Iteration 162, loss = 0.40697855\n",
      "Iteration 163, loss = 0.40658002\n",
      "Iteration 164, loss = 0.40610352\n",
      "Iteration 165, loss = 0.40550746\n",
      "Iteration 166, loss = 0.40487858\n",
      "Iteration 167, loss = 0.40421454\n",
      "Iteration 168, loss = 0.40360864\n",
      "Iteration 169, loss = 0.40307724\n",
      "Iteration 170, loss = 0.40248027\n",
      "Iteration 171, loss = 0.40188414\n",
      "Iteration 172, loss = 0.40128558\n",
      "Iteration 173, loss = 0.40086988\n",
      "Iteration 174, loss = 0.40018682\n",
      "Iteration 175, loss = 0.39973546\n",
      "Iteration 176, loss = 0.39895537\n",
      "Iteration 177, loss = 0.39827755\n",
      "Iteration 178, loss = 0.39788308\n",
      "Iteration 179, loss = 0.39718232\n",
      "Iteration 180, loss = 0.39648287\n",
      "Iteration 181, loss = 0.39588956\n",
      "Iteration 182, loss = 0.39571062\n",
      "Iteration 183, loss = 0.39466545\n",
      "Iteration 184, loss = 0.39432602\n",
      "Iteration 185, loss = 0.39366385\n",
      "Iteration 186, loss = 0.39301240\n",
      "Iteration 187, loss = 0.39255566\n",
      "Iteration 188, loss = 0.39155250\n",
      "Iteration 189, loss = 0.39095499\n",
      "Iteration 190, loss = 0.39027186\n",
      "Iteration 191, loss = 0.38963412\n",
      "Iteration 192, loss = 0.38898068\n",
      "Iteration 193, loss = 0.38838939\n",
      "Iteration 194, loss = 0.38810654\n",
      "Iteration 195, loss = 0.38708464\n",
      "Iteration 196, loss = 0.38679292\n",
      "Iteration 197, loss = 0.38587279\n",
      "Iteration 198, loss = 0.38585054\n",
      "Iteration 199, loss = 0.38427183\n",
      "Iteration 200, loss = 0.38386422\n",
      "Iteration 201, loss = 0.38319604\n",
      "Iteration 202, loss = 0.38238917\n",
      "Iteration 203, loss = 0.38200543\n",
      "Iteration 204, loss = 0.38117647\n",
      "Iteration 205, loss = 0.38043278\n",
      "Iteration 206, loss = 0.37960043\n",
      "Iteration 207, loss = 0.37869060\n",
      "Iteration 208, loss = 0.37820338\n",
      "Iteration 209, loss = 0.37721561\n",
      "Iteration 210, loss = 0.37720240\n",
      "Iteration 211, loss = 0.37594909\n",
      "Iteration 212, loss = 0.37498624\n",
      "Iteration 213, loss = 0.37417195\n",
      "Iteration 214, loss = 0.37340618\n",
      "Iteration 215, loss = 0.37263499\n",
      "Iteration 216, loss = 0.37198622\n",
      "Iteration 217, loss = 0.37102748\n",
      "Iteration 218, loss = 0.37049264\n",
      "Iteration 219, loss = 0.36968439\n",
      "Iteration 220, loss = 0.36853623\n",
      "Iteration 221, loss = 0.36764968\n",
      "Iteration 222, loss = 0.36716655\n",
      "Iteration 223, loss = 0.36602721\n",
      "Iteration 224, loss = 0.36525063\n",
      "Iteration 225, loss = 0.36435072\n",
      "Iteration 226, loss = 0.36328357\n",
      "Iteration 227, loss = 0.36260961\n",
      "Iteration 228, loss = 0.36159384\n",
      "Iteration 229, loss = 0.36086976\n",
      "Iteration 230, loss = 0.35984861\n",
      "Iteration 231, loss = 0.35863607\n",
      "Iteration 232, loss = 0.35772247\n",
      "Iteration 233, loss = 0.35691244\n",
      "Iteration 234, loss = 0.35599197\n",
      "Iteration 235, loss = 0.35479148\n",
      "Iteration 236, loss = 0.35389512\n",
      "Iteration 237, loss = 0.35285157\n",
      "Iteration 238, loss = 0.35179608\n",
      "Iteration 239, loss = 0.35097470\n",
      "Iteration 240, loss = 0.34963527\n",
      "Iteration 241, loss = 0.34876673\n",
      "Iteration 242, loss = 0.34749508\n",
      "Iteration 243, loss = 0.34647556\n",
      "Iteration 244, loss = 0.34528817\n",
      "Iteration 245, loss = 0.34418060\n",
      "Iteration 246, loss = 0.34358816\n",
      "Iteration 247, loss = 0.34180291\n",
      "Iteration 248, loss = 0.34073108\n",
      "Iteration 249, loss = 0.33950093\n",
      "Iteration 250, loss = 0.33858558\n",
      "Iteration 251, loss = 0.33714189\n",
      "Iteration 252, loss = 0.33581169\n",
      "Iteration 253, loss = 0.33496929\n",
      "Iteration 254, loss = 0.33349782\n",
      "Iteration 255, loss = 0.33213860\n",
      "Iteration 256, loss = 0.33096391\n",
      "Iteration 257, loss = 0.32969197\n",
      "Iteration 258, loss = 0.32829331\n",
      "Iteration 259, loss = 0.32702458\n",
      "Iteration 260, loss = 0.32551805\n",
      "Iteration 261, loss = 0.32421328\n",
      "Iteration 262, loss = 0.32282068\n",
      "Iteration 263, loss = 0.32175598\n",
      "Iteration 264, loss = 0.31999299\n",
      "Iteration 265, loss = 0.31860740\n",
      "Iteration 266, loss = 0.31714061\n",
      "Iteration 267, loss = 0.31597076\n",
      "Iteration 268, loss = 0.31455529\n",
      "Iteration 269, loss = 0.31278527\n",
      "Iteration 270, loss = 0.31161668\n",
      "Iteration 271, loss = 0.30992300\n",
      "Iteration 272, loss = 0.30848990\n",
      "Iteration 273, loss = 0.30684773\n",
      "Iteration 274, loss = 0.30539636\n",
      "Iteration 275, loss = 0.30406571\n",
      "Iteration 276, loss = 0.30242767\n",
      "Iteration 277, loss = 0.30112177\n",
      "Iteration 278, loss = 0.29884789\n",
      "Iteration 279, loss = 0.29736430\n",
      "Iteration 280, loss = 0.29608643\n",
      "Iteration 281, loss = 0.29414410\n",
      "Iteration 282, loss = 0.29266848\n",
      "Iteration 283, loss = 0.29097395\n",
      "Iteration 284, loss = 0.28948942\n",
      "Iteration 285, loss = 0.28791818\n",
      "Iteration 286, loss = 0.28665106\n",
      "Iteration 287, loss = 0.28416323\n",
      "Iteration 288, loss = 0.28251094\n",
      "Iteration 289, loss = 0.28094521\n",
      "Iteration 290, loss = 0.27967994\n",
      "Iteration 291, loss = 0.27748104\n",
      "Iteration 292, loss = 0.27622035\n",
      "Iteration 293, loss = 0.27422543\n",
      "Iteration 294, loss = 0.27235746\n",
      "Iteration 295, loss = 0.27110089\n",
      "Iteration 296, loss = 0.26910904\n",
      "Iteration 297, loss = 0.26725991\n",
      "Iteration 298, loss = 0.26580068\n",
      "Iteration 299, loss = 0.26419734\n",
      "Iteration 300, loss = 0.26183117\n",
      "Iteration 301, loss = 0.26049936\n",
      "Iteration 302, loss = 0.25846041\n",
      "Iteration 303, loss = 0.25693232\n",
      "Iteration 304, loss = 0.25521419\n",
      "Iteration 305, loss = 0.25334215\n",
      "Iteration 306, loss = 0.25160142\n",
      "Iteration 307, loss = 0.24987133\n",
      "Iteration 308, loss = 0.24819363\n",
      "Iteration 309, loss = 0.24676666\n",
      "Iteration 310, loss = 0.24457879\n",
      "Iteration 311, loss = 0.24300330\n",
      "Iteration 312, loss = 0.24122231\n",
      "Iteration 313, loss = 0.23997860\n",
      "Iteration 314, loss = 0.23791120\n",
      "Iteration 315, loss = 0.23630796\n",
      "Iteration 316, loss = 0.23463063\n",
      "Iteration 317, loss = 0.23388380\n",
      "Iteration 318, loss = 0.23109621\n",
      "Iteration 319, loss = 0.22945281\n",
      "Iteration 320, loss = 0.22878759\n",
      "Iteration 321, loss = 0.22633911\n",
      "Iteration 322, loss = 0.22423221\n",
      "Iteration 323, loss = 0.22346123\n",
      "Iteration 324, loss = 0.22106112\n",
      "Iteration 325, loss = 0.21950990\n",
      "Iteration 326, loss = 0.21798472\n",
      "Iteration 327, loss = 0.21699235\n",
      "Iteration 328, loss = 0.21480386\n",
      "Iteration 329, loss = 0.21443518\n",
      "Iteration 330, loss = 0.21189828\n",
      "Iteration 331, loss = 0.20965221\n",
      "Iteration 332, loss = 0.20780971\n",
      "Iteration 333, loss = 0.20662063\n",
      "Iteration 334, loss = 0.20516004\n",
      "Iteration 335, loss = 0.20394914\n",
      "Iteration 336, loss = 0.20268981\n",
      "Iteration 337, loss = 0.20060284\n",
      "Iteration 338, loss = 0.19965003\n",
      "Iteration 339, loss = 0.19713047\n",
      "Iteration 340, loss = 0.19598111\n",
      "Iteration 341, loss = 0.19449742\n",
      "Iteration 342, loss = 0.19428010\n",
      "Iteration 343, loss = 0.19149112\n",
      "Iteration 344, loss = 0.18990923\n",
      "Iteration 345, loss = 0.18856405\n",
      "Iteration 346, loss = 0.18725386\n",
      "Iteration 347, loss = 0.18647208\n",
      "Iteration 348, loss = 0.18440865\n",
      "Iteration 349, loss = 0.18303601\n",
      "Iteration 350, loss = 0.18229935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FabianFranz/Development/Projects/velpTec/Tensorflow/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(5, 5), max_iter=350,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(5, 5), max_iter=350,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, hidden_layer_sizes=(5, 5), max_iter=350,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Das neuronale Netz wird mit den Trainingsdaten traniert\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsergebnis: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Das Ergebnis des Training wird ausgegeben\n",
    "print(\"Trainingsergebnis: %5.3f\" % mlp.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird mit den Testdatensdaten evaluiert\n",
    "predictions = mlp.predict(X_test)\n",
    "# und die Konfusionsmatrix ausgegeben\n",
    "print(confusion_matrix(y_test,predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       1.00      1.00      1.00        11\n",
      "         2.0       1.00      1.00      1.00        12\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aus der Konfusionsmatrix werden precison, recall und f1-score berechnet und ausgebenen\n",
    "print(classification_report(y_test,predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testergebnis: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird getest und das Ergebnis ausgegeben\n",
    "print(\"Testergebnis: %5.3f\" % mlp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS: [array([[-2.50192399e-01,  7.30649192e-01, -1.64974991e-24,\n",
      "        -2.34579155e-01,  3.35396762e-01],\n",
      "       [-7.80883149e-01,  3.04546601e-01, -3.77462673e-90,\n",
      "         6.64201244e-01,  4.99912617e-01],\n",
      "       [ 1.21340414e+00, -3.41279551e-01, -6.32706057e-15,\n",
      "        -6.90372685e-01, -9.26775873e-02],\n",
      "       [ 1.20569598e+00, -6.49905302e-01,  3.61997463e-25,\n",
      "         7.21314323e-01, -2.15356530e-01]]), array([[-1.45297636e+00,  2.23431925e-01,  1.36650078e+00,\n",
      "        -2.15921949e-01,  6.31799580e-86],\n",
      "       [ 5.74775521e-01,  2.56348220e-01, -5.25004737e-01,\n",
      "         5.40111568e-01, -1.61639657e-79],\n",
      "       [-5.77390469e-15,  1.67515420e-30, -4.07272939e-95,\n",
      "        -1.28363116e-95,  1.26257054e-13],\n",
      "       [-1.80852785e-01,  6.05279618e-01,  4.94363861e-01,\n",
      "         3.49419786e-01, -2.20859691e-34],\n",
      "       [ 5.65580413e-02, -8.02755303e-01, -4.01696949e-01,\n",
      "        -4.79534245e-01,  5.79129562e-79]]), array([[ 1.19508973e+00, -1.34511114e+00, -5.94048127e-01],\n",
      "       [-5.79365054e-01, -1.02186344e-01,  4.71658627e-01],\n",
      "       [-1.65000202e+00, -5.14190355e-01,  4.41625671e-01],\n",
      "       [ 3.97865163e-01, -6.60688565e-01,  5.18744657e-01],\n",
      "       [-9.81233962e-91,  2.86798595e-80, -1.67760979e-85]])]\n",
      "BIASES: [array([ 0.16745364,  0.72638609, -0.66797373, -0.75057216,  0.70161858]), array([ 0.56172776, -0.56310122, -0.10433709, -0.27840326, -0.10808888]), array([-0.79036424,  1.03900488, -1.17907741])]\n"
     ]
    }
   ],
   "source": [
    "# Folgendes gibt die Werte der Gewichte pro Layer aus\n",
    "print(\"WEIGHTS:\", mlp.coefs_)\n",
    "print(\"BIASES:\", mlp.intercepts_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird beispielsweise zur Vorhersage auf folgenden Werten \n",
    "# aus dem Testset angewandt mit den Merkmalen [sepal-length, sepal-width, \n",
    "# petal-length, petal-width]\n",
    "print(mlp.predict([[5.1,3.5,1.4,0.2], [5.9,3.,5.1,1.8], [4.9,3.,1.4,0.2], [5.8,2.7,4.1,1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Die Loss-Kurve wird visualisiert und in der Datei Plot_of_loss_values.png im PNG-Format gespeichert.\n",
    "loss_values = mlp.loss_curve_\n",
    "plt.plot(loss_values)\n",
    "# Check, if there is already a file with the same name\n",
    "# If yes, add a number to the filename\n",
    "i = 0\n",
    "while os.path.isfile(\"./output/plot_of_loss_values.png\"):\n",
    "    i += 1\n",
    "    plt.savefig(\"./output/plot_of_loss_values\" + str(i) + \".png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
