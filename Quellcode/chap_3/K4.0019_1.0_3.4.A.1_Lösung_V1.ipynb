{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Ein Einfaches Neuronales Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash:  2407\n"
     ]
    }
   ],
   "source": [
    "# Generate random hash\n",
    "hash = random.getrandbits(16)\n",
    "print(\"Hash: \", hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lade den Iris-Datenset\n",
    "data_train = pd.read_csv('./input/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die 3 zu erkennenden Klassifikationsklassen werden zu numerischen Werten 0, 1 bzw. 2 umgewandelt.\n",
    "data_train.loc[data_train['species']=='Iris-setosa', 'species']=0\n",
    "data_train.loc[data_train['species']=='Iris-versicolor', 'species']=1\n",
    "data_train.loc[data_train['species']=='Iris-virginica', 'species']=2\n",
    "data_train = data_train.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der eingelesene Datenset wird als Matrix dargestellt\n",
    "data_train_array = data_train.values # oder data_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zur Sicherstellung der Reproduzierbarkeit der Ergebnisse setzen wir random.seed auf eine festen Wert, z.B. 42\n",
    "np.random.seed(17)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Datenset wird in zwei separate Kategorie gespaltet: Testdaten und Trainingsdaten. \n",
    "\n",
    "80% der Daten werden zum Trainieren und 20% zum Testen des Modells verwendet. \n",
    "\n",
    "Da es sich bei der Eingabe um einen Vektor handelt, werden wird den Großbuchstaben X benutzen.\n",
    "\n",
    "Für die Ausgabe hingegen handelt es sich um ein einzelner Werte, \n",
    "daher die Bezeichung mit dem Kleinbuchstaben y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_train_array[:,:4],\n",
    "                                                    data_train_array[:,4],\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 1\n",
    "Ein neuronales Netz zur Klassifikation (MultiLayerPerceptron) wird mit folgenden Eigenschaften gebildet:\n",
    "- einem Input-Layer mit 4 Neuronen, die die Merkmale der Iris-Planze repräsentieren\n",
    "- einem Hidden-Layer mit 10 Neuronen\n",
    "- einem Output-Layer mit 3 Neuronen, die die zu erkennenden Klassen repräsentieren\n",
    "\n",
    "Dabei wird als Aktivierungsfunktion relu und als Optimierer adam verwenden.\n",
    "\n",
    "<img src=\"img/network_01.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10,),activation='relu', solver='adam', max_iter=350, batch_size=10, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2\n",
    "Erstelle eine zweite Version des neuronalen Netzes mit folgenden Eigenschaften:\n",
    "- einem Input-Layer mit 4 Neuronen, die die Merkmale der Iris-Planze repräsentieren\n",
    "- zwei Hidden-Layer mit jeweils 3 und 5 Neuronen\n",
    "- einem Output-Layer mit 3 Neuronen, die die zu erkennenden Klassen repräsentieren\n",
    "\n",
    "Füge die Zweite Version direkt unter der Ersten ein. Und führe die neu Zelle und alle folgenden aus.\n",
    "\n",
    "<img src=\"img/network_02.png\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zweite Version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.00250272\n",
      "Iteration 2, loss = 3.40553964\n",
      "Iteration 3, loss = 2.83501057\n",
      "Iteration 4, loss = 2.31506032\n",
      "Iteration 5, loss = 1.83550853\n",
      "Iteration 6, loss = 1.51090872\n",
      "Iteration 7, loss = 1.32850876\n",
      "Iteration 8, loss = 1.24046965\n",
      "Iteration 9, loss = 1.17041371\n",
      "Iteration 10, loss = 1.12110589\n",
      "Iteration 11, loss = 1.07358008\n",
      "Iteration 12, loss = 1.03679092\n",
      "Iteration 13, loss = 1.00474070\n",
      "Iteration 14, loss = 0.97298690\n",
      "Iteration 15, loss = 0.94589371\n",
      "Iteration 16, loss = 0.92051278\n",
      "Iteration 17, loss = 0.89480893\n",
      "Iteration 18, loss = 0.87292932\n",
      "Iteration 19, loss = 0.84832888\n",
      "Iteration 20, loss = 0.82650054\n",
      "Iteration 21, loss = 0.80645614\n",
      "Iteration 22, loss = 0.78835510\n",
      "Iteration 23, loss = 0.76874525\n",
      "Iteration 24, loss = 0.75254667\n",
      "Iteration 25, loss = 0.73259700\n",
      "Iteration 26, loss = 0.71952956\n",
      "Iteration 27, loss = 0.70244477\n",
      "Iteration 28, loss = 0.68857793\n",
      "Iteration 29, loss = 0.67245257\n",
      "Iteration 30, loss = 0.66097361\n",
      "Iteration 31, loss = 0.64636006\n",
      "Iteration 32, loss = 0.63455522\n",
      "Iteration 33, loss = 0.62332761\n",
      "Iteration 34, loss = 0.61314445\n",
      "Iteration 35, loss = 0.60200208\n",
      "Iteration 36, loss = 0.59365949\n",
      "Iteration 37, loss = 0.58603645\n",
      "Iteration 38, loss = 0.57395705\n",
      "Iteration 39, loss = 0.56471821\n",
      "Iteration 40, loss = 0.55661838\n",
      "Iteration 41, loss = 0.54943590\n",
      "Iteration 42, loss = 0.54118233\n",
      "Iteration 43, loss = 0.53311911\n",
      "Iteration 44, loss = 0.52634641\n",
      "Iteration 45, loss = 0.52357651\n",
      "Iteration 46, loss = 0.51608916\n",
      "Iteration 47, loss = 0.50697203\n",
      "Iteration 48, loss = 0.50110485\n",
      "Iteration 49, loss = 0.49573385\n",
      "Iteration 50, loss = 0.48958652\n",
      "Iteration 51, loss = 0.48114664\n",
      "Iteration 52, loss = 0.47741165\n",
      "Iteration 53, loss = 0.46510576\n",
      "Iteration 54, loss = 0.46048672\n",
      "Iteration 55, loss = 0.45040712\n",
      "Iteration 56, loss = 0.44371987\n",
      "Iteration 57, loss = 0.43589618\n",
      "Iteration 58, loss = 0.43091133\n",
      "Iteration 59, loss = 0.42159497\n",
      "Iteration 60, loss = 0.41455269\n",
      "Iteration 61, loss = 0.40858428\n",
      "Iteration 62, loss = 0.40152708\n",
      "Iteration 63, loss = 0.39565112\n",
      "Iteration 64, loss = 0.38891406\n",
      "Iteration 65, loss = 0.38314380\n",
      "Iteration 66, loss = 0.37798783\n",
      "Iteration 67, loss = 0.37205915\n",
      "Iteration 68, loss = 0.36731584\n",
      "Iteration 69, loss = 0.36172866\n",
      "Iteration 70, loss = 0.35600670\n",
      "Iteration 71, loss = 0.35055225\n",
      "Iteration 72, loss = 0.34615675\n",
      "Iteration 73, loss = 0.34143502\n",
      "Iteration 74, loss = 0.33589246\n",
      "Iteration 75, loss = 0.33253429\n",
      "Iteration 76, loss = 0.32701748\n",
      "Iteration 77, loss = 0.32195726\n",
      "Iteration 78, loss = 0.31894041\n",
      "Iteration 79, loss = 0.31385268\n",
      "Iteration 80, loss = 0.31002660\n",
      "Iteration 81, loss = 0.30643779\n",
      "Iteration 82, loss = 0.30218609\n",
      "Iteration 83, loss = 0.29715981\n",
      "Iteration 84, loss = 0.29543140\n",
      "Iteration 85, loss = 0.29061314\n",
      "Iteration 86, loss = 0.28954316\n",
      "Iteration 87, loss = 0.28586530\n",
      "Iteration 88, loss = 0.27882506\n",
      "Iteration 89, loss = 0.27790751\n",
      "Iteration 90, loss = 0.27268603\n",
      "Iteration 91, loss = 0.27053940\n",
      "Iteration 92, loss = 0.26629795\n",
      "Iteration 93, loss = 0.26412826\n",
      "Iteration 94, loss = 0.26324762\n",
      "Iteration 95, loss = 0.25607121\n",
      "Iteration 96, loss = 0.25705462\n",
      "Iteration 97, loss = 0.25020809\n",
      "Iteration 98, loss = 0.25001839\n",
      "Iteration 99, loss = 0.24403819\n",
      "Iteration 100, loss = 0.24379350\n",
      "Iteration 101, loss = 0.24305067\n",
      "Iteration 102, loss = 0.23614715\n",
      "Iteration 103, loss = 0.23382451\n",
      "Iteration 104, loss = 0.23075420\n",
      "Iteration 105, loss = 0.22975345\n",
      "Iteration 106, loss = 0.22583900\n",
      "Iteration 107, loss = 0.22694159\n",
      "Iteration 108, loss = 0.22680338\n",
      "Iteration 109, loss = 0.21763400\n",
      "Iteration 110, loss = 0.21740362\n",
      "Iteration 111, loss = 0.21586797\n",
      "Iteration 112, loss = 0.21190035\n",
      "Iteration 113, loss = 0.21014268\n",
      "Iteration 114, loss = 0.20762820\n",
      "Iteration 115, loss = 0.20484095\n",
      "Iteration 116, loss = 0.20269286\n",
      "Iteration 117, loss = 0.20123277\n",
      "Iteration 118, loss = 0.19833841\n",
      "Iteration 119, loss = 0.19644333\n",
      "Iteration 120, loss = 0.19567437\n",
      "Iteration 121, loss = 0.19342245\n",
      "Iteration 122, loss = 0.19173993\n",
      "Iteration 123, loss = 0.19072406\n",
      "Iteration 124, loss = 0.19032356\n",
      "Iteration 125, loss = 0.18723596\n",
      "Iteration 126, loss = 0.18458529\n",
      "Iteration 127, loss = 0.18270626\n",
      "Iteration 128, loss = 0.18171213\n",
      "Iteration 129, loss = 0.17858243\n",
      "Iteration 130, loss = 0.17747241\n",
      "Iteration 131, loss = 0.17598339\n",
      "Iteration 132, loss = 0.17367914\n",
      "Iteration 133, loss = 0.17382476\n",
      "Iteration 134, loss = 0.17009089\n",
      "Iteration 135, loss = 0.17160257\n",
      "Iteration 136, loss = 0.17062764\n",
      "Iteration 137, loss = 0.16534896\n",
      "Iteration 138, loss = 0.16446967\n",
      "Iteration 139, loss = 0.16582767\n",
      "Iteration 140, loss = 0.16111476\n",
      "Iteration 141, loss = 0.16061014\n",
      "Iteration 142, loss = 0.15994230\n",
      "Iteration 143, loss = 0.15759381\n",
      "Iteration 144, loss = 0.15594770\n",
      "Iteration 145, loss = 0.15606202\n",
      "Iteration 146, loss = 0.15450524\n",
      "Iteration 147, loss = 0.15317786\n",
      "Iteration 148, loss = 0.15146931\n",
      "Iteration 149, loss = 0.15023079\n",
      "Iteration 150, loss = 0.14920274\n",
      "Iteration 151, loss = 0.15225785\n",
      "Iteration 152, loss = 0.14625697\n",
      "Iteration 153, loss = 0.14547466\n",
      "Iteration 154, loss = 0.14421974\n",
      "Iteration 155, loss = 0.14351199\n",
      "Iteration 156, loss = 0.14291335\n",
      "Iteration 157, loss = 0.14455087\n",
      "Iteration 158, loss = 0.14157281\n",
      "Iteration 159, loss = 0.14013531\n",
      "Iteration 160, loss = 0.13834233\n",
      "Iteration 161, loss = 0.13918290\n",
      "Iteration 162, loss = 0.13758428\n",
      "Iteration 163, loss = 0.13471172\n",
      "Iteration 164, loss = 0.13488043\n",
      "Iteration 165, loss = 0.13286354\n",
      "Iteration 166, loss = 0.13289850\n",
      "Iteration 167, loss = 0.13139205\n",
      "Iteration 168, loss = 0.13143697\n",
      "Iteration 169, loss = 0.12990592\n",
      "Iteration 170, loss = 0.12916179\n",
      "Iteration 171, loss = 0.12828003\n",
      "Iteration 172, loss = 0.12725125\n",
      "Iteration 173, loss = 0.12685802\n",
      "Iteration 174, loss = 0.12657482\n",
      "Iteration 175, loss = 0.12432796\n",
      "Iteration 176, loss = 0.12389855\n",
      "Iteration 177, loss = 0.12395765\n",
      "Iteration 178, loss = 0.12293427\n",
      "Iteration 179, loss = 0.12190641\n",
      "Iteration 180, loss = 0.12044426\n",
      "Iteration 181, loss = 0.12085807\n",
      "Iteration 182, loss = 0.11999239\n",
      "Iteration 183, loss = 0.12106574\n",
      "Iteration 184, loss = 0.11784849\n",
      "Iteration 185, loss = 0.11731183\n",
      "Iteration 186, loss = 0.11679709\n",
      "Iteration 187, loss = 0.11582289\n",
      "Iteration 188, loss = 0.11505195\n",
      "Iteration 189, loss = 0.11624004\n",
      "Iteration 190, loss = 0.11365127\n",
      "Iteration 191, loss = 0.11385216\n",
      "Iteration 192, loss = 0.11362652\n",
      "Iteration 193, loss = 0.11320538\n",
      "Iteration 194, loss = 0.11127776\n",
      "Iteration 195, loss = 0.11056493\n",
      "Iteration 196, loss = 0.11019247\n",
      "Iteration 197, loss = 0.10938434\n",
      "Iteration 198, loss = 0.10907792\n",
      "Iteration 199, loss = 0.11094108\n",
      "Iteration 200, loss = 0.10911017\n",
      "Iteration 201, loss = 0.10688460\n",
      "Iteration 202, loss = 0.10677177\n",
      "Iteration 203, loss = 0.10621839\n",
      "Iteration 204, loss = 0.10550440\n",
      "Iteration 205, loss = 0.10540597\n",
      "Iteration 206, loss = 0.10433860\n",
      "Iteration 207, loss = 0.10416424\n",
      "Iteration 208, loss = 0.10373615\n",
      "Iteration 209, loss = 0.10304358\n",
      "Iteration 210, loss = 0.10238204\n",
      "Iteration 211, loss = 0.10178099\n",
      "Iteration 212, loss = 0.10253745\n",
      "Iteration 213, loss = 0.10065243\n",
      "Iteration 214, loss = 0.10139100\n",
      "Iteration 215, loss = 0.10473503\n",
      "Iteration 216, loss = 0.10236394\n",
      "Iteration 217, loss = 0.09989665\n",
      "Iteration 218, loss = 0.09891096\n",
      "Iteration 219, loss = 0.09939591\n",
      "Iteration 220, loss = 0.10119130\n",
      "Iteration 221, loss = 0.09706190\n",
      "Iteration 222, loss = 0.09798006\n",
      "Iteration 223, loss = 0.09707054\n",
      "Iteration 224, loss = 0.09713712\n",
      "Iteration 225, loss = 0.09660875\n",
      "Iteration 226, loss = 0.09663119\n",
      "Iteration 227, loss = 0.09484372\n",
      "Iteration 228, loss = 0.09656171\n",
      "Iteration 229, loss = 0.09458237\n",
      "Iteration 230, loss = 0.09427047\n",
      "Iteration 231, loss = 0.09362427\n",
      "Iteration 232, loss = 0.09308312\n",
      "Iteration 233, loss = 0.09325394\n",
      "Iteration 234, loss = 0.09228941\n",
      "Iteration 235, loss = 0.09241620\n",
      "Iteration 236, loss = 0.09151906\n",
      "Iteration 237, loss = 0.09266828\n",
      "Iteration 238, loss = 0.08962935\n",
      "Iteration 239, loss = 0.09229031\n",
      "Iteration 240, loss = 0.08964163\n",
      "Iteration 241, loss = 0.09028065\n",
      "Iteration 242, loss = 0.08936245\n",
      "Iteration 243, loss = 0.08932396\n",
      "Iteration 244, loss = 0.08876262\n",
      "Iteration 245, loss = 0.08839279\n",
      "Iteration 246, loss = 0.08778860\n",
      "Iteration 247, loss = 0.09186589\n",
      "Iteration 248, loss = 0.08601158\n",
      "Iteration 249, loss = 0.08913588\n",
      "Iteration 250, loss = 0.08830487\n",
      "Iteration 251, loss = 0.08715911\n",
      "Iteration 252, loss = 0.08720084\n",
      "Iteration 253, loss = 0.08595909\n",
      "Iteration 254, loss = 0.08659991\n",
      "Iteration 255, loss = 0.08606938\n",
      "Iteration 256, loss = 0.08532553\n",
      "Iteration 257, loss = 0.08559611\n",
      "Iteration 258, loss = 0.08577490\n",
      "Iteration 259, loss = 0.08557039\n",
      "Iteration 260, loss = 0.08507476\n",
      "Iteration 261, loss = 0.08459227\n",
      "Iteration 262, loss = 0.08612319\n",
      "Iteration 263, loss = 0.08522679\n",
      "Iteration 264, loss = 0.08263237\n",
      "Iteration 265, loss = 0.08287833\n",
      "Iteration 266, loss = 0.08212850\n",
      "Iteration 267, loss = 0.08355807\n",
      "Iteration 268, loss = 0.08439522\n",
      "Iteration 269, loss = 0.08366551\n",
      "Iteration 270, loss = 0.08133942\n",
      "Iteration 271, loss = 0.08152976\n",
      "Iteration 272, loss = 0.08097790\n",
      "Iteration 273, loss = 0.08133977\n",
      "Iteration 274, loss = 0.08143214\n",
      "Iteration 275, loss = 0.08004292\n",
      "Iteration 276, loss = 0.08064937\n",
      "Iteration 277, loss = 0.08015252\n",
      "Iteration 278, loss = 0.07909858\n",
      "Iteration 279, loss = 0.07905402\n",
      "Iteration 280, loss = 0.07860276\n",
      "Iteration 281, loss = 0.07951599\n",
      "Iteration 282, loss = 0.07861664\n",
      "Iteration 283, loss = 0.07909591\n",
      "Iteration 284, loss = 0.07782827\n",
      "Iteration 285, loss = 0.07948777\n",
      "Iteration 286, loss = 0.07705228\n",
      "Iteration 287, loss = 0.07784224\n",
      "Iteration 288, loss = 0.07723424\n",
      "Iteration 289, loss = 0.07692231\n",
      "Iteration 290, loss = 0.07713531\n",
      "Iteration 291, loss = 0.07943792\n",
      "Iteration 292, loss = 0.07640899\n",
      "Iteration 293, loss = 0.07587269\n",
      "Iteration 294, loss = 0.07601615\n",
      "Iteration 295, loss = 0.07640203\n",
      "Iteration 296, loss = 0.07544157\n",
      "Iteration 297, loss = 0.07639115\n",
      "Iteration 298, loss = 0.07644806\n",
      "Iteration 299, loss = 0.07570033\n",
      "Iteration 300, loss = 0.07449521\n",
      "Iteration 301, loss = 0.07520592\n",
      "Iteration 302, loss = 0.07855861\n",
      "Iteration 303, loss = 0.07307318\n",
      "Iteration 304, loss = 0.07550570\n",
      "Iteration 305, loss = 0.07365299\n",
      "Iteration 306, loss = 0.07426593\n",
      "Iteration 307, loss = 0.07338815\n",
      "Iteration 308, loss = 0.07582690\n",
      "Iteration 309, loss = 0.07442961\n",
      "Iteration 310, loss = 0.07351230\n",
      "Iteration 311, loss = 0.07509967\n",
      "Iteration 312, loss = 0.07195877\n",
      "Iteration 313, loss = 0.07312217\n",
      "Iteration 314, loss = 0.07253805\n",
      "Iteration 315, loss = 0.07231421\n",
      "Iteration 316, loss = 0.07167338\n",
      "Iteration 317, loss = 0.07313583\n",
      "Iteration 318, loss = 0.07725933\n",
      "Iteration 319, loss = 0.07099868\n",
      "Iteration 320, loss = 0.07177973\n",
      "Iteration 321, loss = 0.07218251\n",
      "Iteration 322, loss = 0.07257143\n",
      "Iteration 323, loss = 0.07144441\n",
      "Iteration 324, loss = 0.07107552\n",
      "Iteration 325, loss = 0.07030424\n",
      "Iteration 326, loss = 0.07067954\n",
      "Iteration 327, loss = 0.07030101\n",
      "Iteration 328, loss = 0.07029363\n",
      "Iteration 329, loss = 0.06954314\n",
      "Iteration 330, loss = 0.07194073\n",
      "Iteration 331, loss = 0.07057397\n",
      "Iteration 332, loss = 0.07004004\n",
      "Iteration 333, loss = 0.07165578\n",
      "Iteration 334, loss = 0.06940251\n",
      "Iteration 335, loss = 0.07100749\n",
      "Iteration 336, loss = 0.06918533\n",
      "Iteration 337, loss = 0.06916999\n",
      "Iteration 338, loss = 0.07195196\n",
      "Iteration 339, loss = 0.06954407\n",
      "Iteration 340, loss = 0.06860971\n",
      "Iteration 341, loss = 0.06931413\n",
      "Iteration 342, loss = 0.07000790\n",
      "Iteration 343, loss = 0.06912624\n",
      "Iteration 344, loss = 0.06787945\n",
      "Iteration 345, loss = 0.06835903\n",
      "Iteration 346, loss = 0.06762316\n",
      "Iteration 347, loss = 0.07096147\n",
      "Iteration 348, loss = 0.06972997\n",
      "Iteration 349, loss = 0.07082944\n",
      "Iteration 350, loss = 0.06655910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/FabianFranz/Development/Projects/velpTec/Tensorflow/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (350) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(10,), max_iter=350,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=10, hidden_layer_sizes=(10,), max_iter=350,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=10, hidden_layer_sizes=(10,), max_iter=350,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Das neuronale Netz wird mit den Trainingsdaten traniert\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsergebnis: 0.983\n"
     ]
    }
   ],
   "source": [
    "# Das Ergebnis des Training wird ausgegeben\n",
    "print(\"Trainingsergebnis: %5.3f\" % mlp.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7  0  0]\n",
      " [ 0 11  0]\n",
      " [ 0  1 11]]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird mit den Testdatensdaten evaluiert\n",
    "predictions = mlp.predict(X_test)\n",
    "# und die Konfusionsmatrix ausgegeben\n",
    "print(confusion_matrix(y_test,predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         7\n",
      "         1.0       0.92      1.00      0.96        11\n",
      "         2.0       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aus der Konfusionsmatrix werden precison, recall und f1-score berechnet und ausgebenen\n",
    "print(classification_report(y_test,predictions)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testergebnis: 0.967\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird getest und das Ergebnis ausgegeben\n",
    "print(\"Testergebnis: %5.3f\" % mlp.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS: [array([[-2.82928616e-02,  3.98626014e-01, -5.97650566e-02,\n",
      "         3.60177468e-01,  8.35854375e-02,  4.61691767e-67,\n",
      "         5.58140154e-01,  3.03496513e-01,  3.98363846e-01,\n",
      "        -2.26749801e-02],\n",
      "       [ 6.69030070e-01, -3.14379912e-01, -9.14913685e-02,\n",
      "         5.20526479e-01, -6.07593114e-01, -3.72425974e-98,\n",
      "         3.55825014e-01,  6.43137144e-01,  4.05330555e-01,\n",
      "        -3.71695990e-01],\n",
      "       [ 5.68242922e-01,  6.41685919e-01, -5.92638144e-02,\n",
      "        -5.45713391e-01,  1.42606035e+00, -8.59704250e-22,\n",
      "        -2.29152900e-03,  5.22687734e-01,  1.62946479e-02,\n",
      "         4.20088178e-02],\n",
      "       [ 3.74815797e-01,  5.69459224e-01, -2.24764024e-02,\n",
      "        -6.67575691e-01,  1.20578245e+00, -1.95510383e-20,\n",
      "        -9.93512796e-01, -1.93401619e-01,  2.19345212e-01,\n",
      "         1.08464432e+00]]), array([[ 4.95433091e-01, -4.86281052e-01,  4.44882544e-01],\n",
      "       [-7.49077340e-02, -5.77231816e-02,  3.09369926e-01],\n",
      "       [ 5.35374686e-02, -8.18924816e-03, -5.34071722e-02],\n",
      "       [ 2.16582651e+00,  1.09421618e+00, -2.25413978e+00],\n",
      "       [-2.04139219e+00,  6.33446836e-02,  7.50051333e-01],\n",
      "       [-1.35138137e-22, -8.52400906e-30,  7.23022956e-94],\n",
      "       [ 7.67483959e-01,  3.42160066e-01, -6.39654527e-01],\n",
      "       [-2.53905584e-01,  1.40318999e-01, -7.03359532e-01],\n",
      "       [ 1.04147974e-01,  1.19211175e-01,  2.71911450e-01],\n",
      "       [-3.39491711e-01, -9.52355438e-01,  9.09437781e-01]])]\n",
      "BIASES: [array([-0.46699862,  0.1718756 ,  0.4377293 ,  1.1591752 , -0.80572818,\n",
      "       -0.19448985, -0.02452142,  0.66509048, -0.52902837, -0.31067644]), array([-0.05163483, -0.25541857, -1.0410276 ])]\n"
     ]
    }
   ],
   "source": [
    "# Folgendes gibt die Werte der Gewichte pro Layer aus\n",
    "print(\"WEIGHTS:\", mlp.coefs_)\n",
    "print(\"BIASES:\", mlp.intercepts_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Das Modell wird beispielsweise zur Vorhersage auf folgenden Werten \n",
    "# aus dem Testset angewandt mit den Merkmalen [sepal-length, sepal-width, \n",
    "# petal-length, petal-width]\n",
    "print(mlp.predict([[5.1,3.5,1.4,0.2], [5.9,3.,5.1,1.8], [4.9,3.,1.4,0.2], [5.8,2.7,4.1,1.]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8vklEQVR4nO3de3yU5YH3/++ck5DMJCHkQAiIgghyRoFgFVtRpD6ubLc+lPW32K7aRxe6urrtittqq79t3PVx1a3WQ63SraVYbYGu9VCKBVTiASQVUFEUSYBMOCaT48xk5nr+SDIwkEAmJHOTzOf9et0vknuue+a6ZsZXvl6n22aMMQIAALCI3eoKAACA1EYYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYyml1BbojGo1q3759ysrKks1ms7o6AACgG4wxqq+v19ChQ2W3d93/0S/CyL59+1RSUmJ1NQAAQA9UVVVp2LBhXT7eL8JIVlaWpLbGeL1ei2sDAAC6IxAIqKSkJPZ3vCv9Iox0DM14vV7CCAAA/cypplgwgRUAAFiKMAIAACxFGAEAAJYijAAAAEsRRgAAgKUIIwAAwFKEEQAAYCnCCAAAsBRhBAAAWIowAgAALHVaYeT++++XzWbTbbfddtJyL7zwgs477zylpaVpwoQJevnll0/nZQEAwADS4zDy3nvv6cknn9TEiRNPWm7jxo1auHChbrjhBm3ZskXz58/X/PnztW3btp6+NAAAGEB6FEYaGhp03XXX6Wc/+5lycnJOWvaRRx7RlVdeqe9+97saO3as7rvvPk2dOlWPPvpojyrcm37+5i7ds3qbdvjrra4KAAApq0dhZPHixbrqqqs0Z86cU5YtLy8/odzcuXNVXl7e5TXBYFCBQCDu6AsvfbBPvyjfrd2HGvvk+QEAwKk5E71gxYoVev/99/Xee+91q7zf71dBQUHcuYKCAvn9/i6vKSsr049+9KNEq5Ywt6Mti4Ui0T5/LQAA0LmEekaqqqp066236le/+pXS0tL6qk5aunSp6urqYkdVVVWfvI7b2db8MGEEAADLJNQzsnnzZu3fv19Tp06NnYtEItqwYYMeffRRBYNBORyOuGsKCwtVU1MTd66mpkaFhYVdvo7H45HH40mkaj0S6xlpJYwAAGCVhHpGLrvsMm3dulUVFRWx44ILLtB1112nioqKE4KIJJWWlmrt2rVx59asWaPS0tLTq3kv6OgZIYwAAGCdhHpGsrKyNH78+LhzgwYN0uDBg2PnFy1apOLiYpWVlUmSbr31Vs2ePVsPPvigrrrqKq1YsUKbNm3SU0891UtN6DlXbM6IsbgmAACkrl7fgbWyslLV1dWx32fNmqXly5frqaee0qRJk/Tiiy9q1apVJ4QaK9AzAgCA9RJeTXO8devWnfR3Sbr22mt17bXXnu5L9ToXc0YAALBcSt+bxsNqGgAALJfSYSQ2TEMYAQDAMikdRlwOmySGaQAAsFJKhxF3+1JkekYAALBOaocRVtMAAGC5lA4jHcM0TGAFAMA6KR1GPPSMAABguZQOIwzTAABgvZQOI0e3gyeMAABglZQOI/SMAABgvZQOI/SMAABgvZQOI262gwcAwHIpHUY83CgPAADLpXQYcTFnBAAAy6V0GHE7OoZpjMU1AQAgdaV2GGnvGQnSMwIAgGVSOozEVtO0RiyuCQAAqSulw4jHyTANAABWS+kwEtv0jKW9AABYJqXDSMcwTSRqFInSOwIAgBVSOox09IxIbHwGAIBVUjqMuBy22M+sqAEAwBopHUY69hmR6BkBAMAqKR1GbDZbLJCwCysAANZI6TAiHR2qIYwAAGCNlA8j3LkXAABrEUbYEh4AAEulfBiJbQlPzwgAAJZI+TASG6ahZwQAAEsQRugZAQDAUoQRJ0t7AQCwUkJh5PHHH9fEiRPl9Xrl9XpVWlqqV155pcvyy5Ytk81mizvS0tJOu9K9qaNnhNU0AABYw5lI4WHDhun+++/X6NGjZYzRL37xC11zzTXasmWLzj///E6v8Xq92rFjR+x3m83WaTmrdExgZTUNAADWSCiMXH311XG//9u//Zsef/xxvf32212GEZvNpsLCwp7XsI8d3WeEu/YCAGCFHs8ZiUQiWrFihRobG1VaWtpluYaGBo0YMUIlJSW65pprtH379lM+dzAYVCAQiDv6CnNGAACwVsJhZOvWrcrMzJTH49HNN9+slStXaty4cZ2WHTNmjJ555hmtXr1azz33nKLRqGbNmqU9e/ac9DXKysrk8/liR0lJSaLV7Laj96aJ9NlrAACArtmMMQmNT4RCIVVWVqqurk4vvviinn76aa1fv77LQHKscDissWPHauHChbrvvvu6LBcMBhUMBmO/BwIBlZSUqK6uTl6vN5HqntI/PV+hlVv26l+/OlY3XXJ2rz43AACpLBAIyOfznfLvd0JzRiTJ7XZr1KhRkqRp06bpvffe0yOPPKInn3zylNe6XC5NmTJFO3fuPGk5j8cjj8eTaNV6hH1GAACw1mnvMxKNRuN6MU4mEolo69atKioqOt2X7TUuZ9vqHlbTAABgjYR6RpYuXap58+Zp+PDhqq+v1/Lly7Vu3Tq99tprkqRFixapuLhYZWVlkqR7771XM2fO1KhRo1RbW6sHHnhAu3fv1o033tj7Lekht8MhiX1GAACwSkJhZP/+/Vq0aJGqq6vl8/k0ceJEvfbaa7r88sslSZWVlbLbj3a2HDlyRDfddJP8fr9ycnI0bdo0bdy4sVvzS5Klo2eE1TQAAFgjoTDy85///KSPr1u3Lu73hx56SA899FDClUomj4OlvQAAWIl70zjZDh4AACulfBhx0TMCAIClUj6MdPSMBOkZAQDAEoSRjmEaekYAALBEyocRF5ueAQBgqZQPIx4msAIAYKmUDyNuJrACAGCplA8jrKYBAMBaKR9GOiawhiIJ3bwYAAD0kpQPI0d7RiIW1wQAgNSU8mHkaM8IwzQAAFgh5cNIbDVNK8M0AABYIeXDCPuMAABgrZQPI7FhGlbTAABgCcIIc0YAALBUyocRl8Mmqa1nxBjmjQAAkGwpH0Y8Dkfs5zB7jQAAkHQpH0ZcTlvsZ+5PAwBA8qV8GOm4N43EJFYAAKyQ8mHE6bDL3t45Qs8IAADJl/JhRDq610iQnhEAAJKOMCKW9wIAYCXCiI7ZEp4wAgBA0hFGdOydewkjAAAkG2FEbAkPAICVCCM6uryXOSMAACQfYUQM0wAAYCXCiBimAQDASoQRHR2m4d40AAAkH2FEx+4zErG4JgAApB7CiBimAQDASoQRSS5H281pQgzTAACQdAmFkccff1wTJ06U1+uV1+tVaWmpXnnllZNe88ILL+i8885TWlqaJkyYoJdffvm0KtwX3E6HJHpGAACwQkJhZNiwYbr//vu1efNmbdq0SV/5yld0zTXXaPv27Z2W37hxoxYuXKgbbrhBW7Zs0fz58zV//nxt27atVyrfW45OYCWMAACQbDZjzGmNTeTm5uqBBx7QDTfccMJjCxYsUGNjo1566aXYuZkzZ2ry5Ml64oknuv0agUBAPp9PdXV18nq9p1PdTi393Qf69btVuv3yc/WPl43u9ecHACAVdffvd4/njEQiEa1YsUKNjY0qLS3ttEx5ebnmzJkTd27u3LkqLy8/6XMHg0EFAoG4oy+52fQMAADLJBxGtm7dqszMTHk8Ht18881auXKlxo0b12lZv9+vgoKCuHMFBQXy+/0nfY2ysjL5fL7YUVJSkmg1E+JimAYAAMskHEbGjBmjiooKvfPOO7rlllt0/fXX68MPP+zVSi1dulR1dXWxo6qqqlef/3gdS3uD9IwAAJB0zkQvcLvdGjVqlCRp2rRpeu+99/TII4/oySefPKFsYWGhampq4s7V1NSosLDwpK/h8Xjk8XgSrVqPHd30jDACAECynfY+I9FoVMFgsNPHSktLtXbt2rhza9as6XKOiVViwzT0jAAAkHQJ9YwsXbpU8+bN0/Dhw1VfX6/ly5dr3bp1eu211yRJixYtUnFxscrKyiRJt956q2bPnq0HH3xQV111lVasWKFNmzbpqaee6v2WnAYPPSMAAFgmoTCyf/9+LVq0SNXV1fL5fJo4caJee+01XX755ZKkyspK2e1HO1tmzZql5cuX6/vf/77uuusujR49WqtWrdL48eN7txWnie3gAQCwTkJh5Oc///lJH1+3bt0J56699lpde+21CVUq2VhNAwCAdbg3jY7uM8JqGgAAko8wIoZpAACwEmFEDNMAAGAlwohYTQMAgJUIIzp2n5HTumcgAADoAcKI2IEVAAArEUbEBFYAAKxEGBFLewEAsBJhRJLH1dEzErG4JgAApB7CiOgZAQDASoQRHe0ZCbZGZQwragAASCbCiCSP0xH7mRU1AAAkF2FERzc9k1hRAwBAshFGdHTOiMS8EQAAko0wIslutzGJFQAAixBG2nUM1QTDLO8FACCZCCPtYnuNMIEVAICkIoy061hREwwTRgAASCbCSLuO+9MwZwQAgOQijLSLzRlhS3gAAJKKMNLOw517AQCwBGGkXWzOCGEEAICkIoy0czNMAwCAJQgj7Y7uM0LPCAAAyUQYacc+IwAAWIMw0o59RgAAsAZhpN3Re9MwZwQAgGQijLTrGKZhNQ0AAMlFGGnHPiMAAFiDMNKOfUYAALAGYaQd+4wAAGANwkg79hkBAMAaCYWRsrIyXXjhhcrKylJ+fr7mz5+vHTt2nPSaZcuWyWazxR1paWmnVem+EAsj7DMCAEBSJRRG1q9fr8WLF+vtt9/WmjVrFA6HdcUVV6ixsfGk13m9XlVXV8eO3bt3n1al+4LHxT4jAABYwZlI4VdffTXu92XLlik/P1+bN2/WJZdc0uV1NptNhYWFPathkrDPCAAA1jitOSN1dXWSpNzc3JOWa2ho0IgRI1RSUqJrrrlG27dvP2n5YDCoQCAQd/Q19hkBAMAaPQ4j0WhUt912my666CKNHz++y3JjxozRM888o9WrV+u5555TNBrVrFmztGfPni6vKSsrk8/nix0lJSU9rWa3dSztZZ8RAACSy2aMMT258JZbbtErr7yiN998U8OGDev2deFwWGPHjtXChQt13333dVomGAwqGAzGfg8EAiopKVFdXZ28Xm9PqntKGz45oEXPvKuxRV69cuvFffIaAACkkkAgIJ/Pd8q/3wnNGemwZMkSvfTSS9qwYUNCQUSSXC6XpkyZop07d3ZZxuPxyOPx9KRqPcY+IwAAWCOhYRpjjJYsWaKVK1fq9ddf18iRIxN+wUgkoq1bt6qoqCjha/sS+4wAAGCNhHpGFi9erOXLl2v16tXKysqS3++XJPl8PqWnp0uSFi1apOLiYpWVlUmS7r33Xs2cOVOjRo1SbW2tHnjgAe3evVs33nhjLzfl9MTmjLDPCAAASZVQGHn88cclSZdeemnc+WeffVbf/OY3JUmVlZWy2492uBw5ckQ33XST/H6/cnJyNG3aNG3cuFHjxo07vZr3sthqmjDDNAAAJFNCYaQ7c13XrVsX9/tDDz2khx56KKFKWeHoPiP0jAAAkEzcm6bdsfuM9HCBEQAA6AHCSLuOOSOSFI4QRgAASBbCSLuO1TQSy3sBAEgmwki7jjkjEvNGAABIJsJIO7vdxiRWAAAsQBg5RsdQDfenAQAgeQgjxzi6ooY5IwAAJAth5BgdK2rYEh4AgOQhjBzj6M3yCCMAACQLYeQYzBkBACD5CCPHiN25lzkjAAAkDWHkGLE5I/SMAACQNISRY7jpGQEAIOkII8dgzggAAMlHGDnGsXfuBQAAyUEYOQb7jAAAkHyEkWMcvTcNc0YAAEgWwsgxOoZpmDMCAEDyEEaO4WEHVgAAko4wcgz2GQEAIPkII8dgnxEAAJKPMHIMhmkAAEg+wsgxCCMAACQfYeQYHhf7jAAAkGyEkWOwzwgAAMlHGDkG+4wAAJB8hJFjsLQXAIDkI4wcgwmsAAAkH2HkGB37jISYMwIAQNIQRo5BzwgAAMlHGDkGc0YAAEi+hMJIWVmZLrzwQmVlZSk/P1/z58/Xjh07TnndCy+8oPPOO09paWmaMGGCXn755R5XuC91rKYJhhmmAQAgWRIKI+vXr9fixYv19ttva82aNQqHw7riiivU2NjY5TUbN27UwoULdcMNN2jLli2aP3++5s+fr23btp125Xtbxz4joQg9IwAAJIvNGGN6evGBAweUn5+v9evX65JLLum0zIIFC9TY2KiXXnopdm7mzJmaPHmynnjiiW69TiAQkM/nU11dnbxeb0+re0r761s0/d/WymaTPv/xV2Wz2frstQAAGOi6+/f7tOaM1NXVSZJyc3O7LFNeXq45c+bEnZs7d67Ky8u7vCYYDCoQCMQdydAxZ8QYKRzpcUYDAAAJ6HEYiUajuu2223TRRRdp/PjxXZbz+/0qKCiIO1dQUCC/39/lNWVlZfL5fLGjpKSkp9VMSMdqGokt4QEASJYeh5HFixdr27ZtWrFiRW/WR5K0dOlS1dXVxY6qqqpef43OdMwZkdgSHgCAZHH25KIlS5bopZde0oYNGzRs2LCTli0sLFRNTU3cuZqaGhUWFnZ5jcfjkcfj6UnVTovdbpPbYVcoEmV5LwAASZJQz4gxRkuWLNHKlSv1+uuva+TIkae8prS0VGvXro07t2bNGpWWliZW0yRh4zMAAJIroZ6RxYsXa/ny5Vq9erWysrJi8z58Pp/S09MlSYsWLVJxcbHKysokSbfeeqtmz56tBx98UFdddZVWrFihTZs26amnnurlpvQOj8uu+iBzRgAASJaEekYef/xx1dXV6dJLL1VRUVHseP7552NlKisrVV1dHft91qxZWr58uZ566ilNmjRJL774olatWnXSSa9W6lhRw5wRAACSI6Geke5sSbJu3boTzl177bW69tprE3kpy7gZpgEAIKm4N81xYnNGwoQRAACSgTBynKMTWJkzAgBAMhBGjsOcEQAAkoswchzmjAAAkFyEkeMwTAMAQHIRRo7jcdEzAgBAMhFGjsOcEQAAkoswcpyOYZqWMMM0AAAkA2HkOOnutp6RphBhBACAZCCMHGeQu21TWsIIAADJQRg5ToanrWekMdhqcU0AAEgNhJHj0DMCAEByEUaOk9E+Z6QxRM8IAADJQBg5ziBPe89IkJ4RAACSgTByHHpGAABILsLIcWI9I8wZAQAgKQgjx4n1jLCaBgCApCCMHIfVNAAAJBdh5DixfUZCrTLGWFwbAAAGPsLIcTp6Rozhzr0AACQDYeQ46S5H7GfmjQAA0PcII8ex222xQMK8EQAA+h5hpBODPOw1AgBAshBGOpHRPm+kkV1YAQDoc4SRTnTsNdJEzwgAAH2OMNKJjl1Y6RkBAKDvEUY6Qc8IAADJQxjpRMdeI42spgEAoM8RRjrRsQtrE/uMAADQ5wgjnaBnBACA5CGMdIKeEQAAkocw0gl6RgAASJ6Ew8iGDRt09dVXa+jQobLZbFq1atVJy69bt042m+2Ew+/397TOfa5jNU0zq2kAAOhzCYeRxsZGTZo0SY899lhC1+3YsUPV1dWxIz8/P9GXTprYPiP0jAAA0OeciV4wb948zZs3L+EXys/PV3Z2dsLXWYF9RgAASJ6kzRmZPHmyioqKdPnll+utt946adlgMKhAIBB3JFNWWltGCzQTRgAA6Gt9HkaKior0xBNP6Le//a1++9vfqqSkRJdeeqnef//9Lq8pKyuTz+eLHSUlJX1dzTi5gzySpMONoaS+LgAAqchmjDE9vthm08qVKzV//vyErps9e7aGDx+uX/7yl50+HgwGFQwGY78HAgGVlJSorq5OXq+3p9XttqrDTbr4P/4sj9Ouj++7Ujabrc9fEwCAgSYQCMjn853y73fCc0Z6w/Tp0/Xmm292+bjH45HH40lijeLlDnJLkoKtUTWFIrEJrQAAoPdZss9IRUWFioqKrHjpbslwO+Rxtr01DNUAANC3Ev5f/oaGBu3cuTP2+65du1RRUaHc3FwNHz5cS5cu1d69e/Xf//3fkqSHH35YI0eO1Pnnn6+WlhY9/fTTev311/XHP/6x91rRy2w2m/IyPdpb26yDDUGV5GZYXSUAAAashMPIpk2b9OUvfzn2++233y5Juv7667Vs2TJVV1ersrIy9ngoFNIdd9yhvXv3KiMjQxMnTtSf/vSnuOc4E+UOcmtvbTM9IwAA9LHTmsCaLN2dANObrn/mXa3/5ID+4+sT9b8vSO5qHgAABoLu/v3m3jRdGNw+iZWeEQAA+hZhpAuDM9vCyKGG4ClKAgCA00EY6ULHxmeH6BkBAKBPEUa6wDANAADJQRjpQsfGZ4caCCMAAPQlwkgXOuaM0DMCAEDfIox0YXBszggTWAEA6EuEkS7ktveMtISjagi2WlwbAAAGLsJIFzI9TuW1B5LPDzRYXBsAAAYuwshJnFuQJUna4a+3uCYAAAxchJGT6Agjn9QQRgAA6CuEkZMYU9jeM1LDMA0AAH2FMHISR4dpAhbXBACAgYswchLnFmRKkmoCQdU2sd8IAAB9gTByEllpLhVnp0uSPmGoBgCAPkEYOYWOeSPb99VZXBMAAAYmwsgpTC7JliRVVNVaWg8AAAYqwsgpEEYAAOhbhJFTmNQeRnYfatKhBu5TAwBAbyOMnIIv3aWzhwySJP1lT621lQEAYAAijHTDlJIcSdKWylprKwIAwABEGOmGaSPawshbOw9aXBMAAAYewkg3fOW8fEnSlqpa7Q+0WFwbAAAGFsJINxT60jSpJFvGSH/6aL/V1QEAYEAhjHTTFeMKJEl//NBvcU0AABhYCCPdNPf8tjCycech1beELa4NAAADB2Gkm84Zkqmz8wYpFIlq/ScHrK4OAAADBmGkm2w2my5v7x354/Yai2sDAMDAQRhJwBXjCiVJf/54v0KtUYtrAwDAwEAYScCUkmwNyfKoPtiqP+9gVQ0AAL2BMJIAu92mr00tliT96p1Ki2sDAMDAQBhJ0HXTR8hmkzZ8ckBfHGy0ujoAAPR7CYeRDRs26Oqrr9bQoUNls9m0atWqU16zbt06TZ06VR6PR6NGjdKyZct6UNUzw/DBGZp97hBJ0i/Kv7C2MgAADAAJh5HGxkZNmjRJjz32WLfK79q1S1dddZW+/OUvq6KiQrfddptuvPFGvfbaawlX9kzx9xeNlCSteLdKhxtDFtcGAID+zZnoBfPmzdO8efO6Xf6JJ57QyJEj9eCDD0qSxo4dqzfffFMPPfSQ5s6dm+jLnxEuHp2n8cVebdsb0LK3dun2K8ZYXSUAAPqtPp8zUl5erjlz5sSdmzt3rsrLy7u8JhgMKhAIxB1nEpvNpsWXjpIkLdv4BTuyAgBwGvo8jPj9fhUUFMSdKygoUCAQUHNzc6fXlJWVyefzxY6SkpK+rmbC5p5fqLOHDFKgpVXLWVkDAECPnZGraZYuXaq6urrYUVVVZXWVTmC323TL7HMkST97Y5eaQxGLawQAQP/U52GksLBQNTXx26fX1NTI6/UqPT2902s8Ho+8Xm/ccSaaP6VYw3LSdbAhqCfWf2Z1dQAA6Jf6PIyUlpZq7dq1cefWrFmj0tLSvn7pPudy2LV03lhJ0pMbPtO+2s6HnQAAQNcSDiMNDQ2qqKhQRUWFpLaluxUVFaqsbJs3sXTpUi1atChW/uabb9bnn3+u733ve/r444/105/+VL/5zW/0T//0T73TAot9dUKhpp+Vq5ZwVPe/8rHV1QEAoN9JOIxs2rRJU6ZM0ZQpUyRJt99+u6ZMmaK7775bklRdXR0LJpI0cuRI/eEPf9CaNWs0adIkPfjgg3r66af77bLe49lsNt199TjZbNLv/7JPm744bHWVAADoV2zGGGN1JU4lEAjI5/Oprq7ujJ0/svR3H+jX71ZpfLFXqxd/SQ67zeoqAQBgqe7+/T4jV9P0R3dcMUZZaU5t2xvQ8++deat/AAA4UxFGeklepkf/NOdcSdJ/vPax9gdaLK4RAAD9A2GkFy0qHaHzh3pV2xTW9377gfrBCBgAAJYjjPQip8OuhxZMlttp17odB/Tb9/daXSUAAM54hJFedm5Blm6bM1qS9OOXP1JtE3f1BQDgZAgjfeDGL52t0fmZOtwY0g9/v93q6gAAcEYjjPQBt9Ou+/9mguw2aVXFPq3cssfqKgEAcMYijPSRaSNydetlbatrvr9ym3YfarS4RgAAnJkII31oyVdGafpZuWoMRfSPKyoUbOXOvgAAHI8w0occdpse+sZkedOc+ktVre5ZvZ3lvgAAHIcw0seKs9P1yMIpstukFe9V6b/Ld1tdJQAAziiEkST48ph8LZ03VpJ070sf6q2dBy2uEQAAZw7CSJLcePFIfW1KsSJRo3/41ftMaAUAoB1hJElsNpt+/LUJmlSSrbrmsG78xSbVt4StrhYAAJYjjCRRmsuhp/5umgq8Hn26v0G3rahQJMqEVgBAaiOMJFmBN01P/d0FcjvtWvvxfv2fX25WU6jV6moBAGAZwogFJpVk67++MUVup11/+qhG3/7vzQq1Rq2uFgAAliCMWOTK8YX69U0zlOF26M2dB3XHC39ROEIgAQCkHsKIhaaNyNVPr5sqh92m//nLPt38y83s0goASDmEEYtdOiZfT/5/0+Rpn0PyneVb1EoPCQAghRBGzgBzxhXomW9eKLfTrj9+WKO/+/m72l/fYnW1AABICsLIGeKiUXl6/LqpynA7VP75IX3tpxtVdbjJ6moBANDnCCNnkMvGFuj3S76kEYMztOdIs77+xEZt3n3Y6moBANCnCCNnmFH5mfrN/ynV6PxM1QSCWvDk23r6jc+52y8AYMAijJyBCrxpWrn4Il09aahao0b//x8+0jeffU97a5utrhoAAL2OMHKGyvQ49V/fmKz75o+X22nX+k8O6Ir/XK9fvr1bUbaQBwAMIISRM5jNZtPfzRyhV269WBeMyFFjKKIfrNqmhT97Wx9VB6yuHgAAvYIw0g+cM6RtHskPrx6nDLdD7+w6rHmPvKF/efEDtYTZJA0A0L8RRvoJu92mb140Uq/ddomunjRUNpv0/KYq/fVPWXEDAOjfbKYfLNMIBALy+Xyqq6uT1+u1ujpnhI07D2rx8vd1pCksSfqrSUP13bljVJKbYXHNAABo092/34SRfuxAfVD/97Ud+s3mKhkjuRw2/e8LSrTkK6NU5Eu3unoAgBTX3b/fPRqmeeyxx3TWWWcpLS1NM2bM0Lvvvttl2WXLlslms8UdaWlpPXlZHGdIlkf//vWJ+p8lX9KXRuUpHDH61TuVmv0f6/T9VVtZCgwA6BeciV7w/PPP6/bbb9cTTzyhGTNm6OGHH9bcuXO1Y8cO5efnd3qN1+vVjh07Yr/bbLae1xgnGF/s03M3ztDbnx/Sf675RO/uOqzn3q7UinerdNGoPE0fmavrZgxXdobb6qoCAHCChIdpZsyYoQsvvFCPPvqoJCkajaqkpETf+c53dOedd55QftmyZbrttttUW1vb40oyTJOYtz8/pP9a+6k2fnYodi7T49SSr4zS3180Um4n85YBAH2vu3+/E+oZCYVC2rx5s5YuXRo7Z7fbNWfOHJWXl3d5XUNDg0aMGKFoNKqpU6fqxz/+sc4///wuyweDQQWDwbjGoPtmnj1YM88erO376rR59xH9+t0qfVQd0P2vfKyn3/hcfz2lWNdeUKJzC7KsrioAAInNGTl48KAikYgKCgrizhcUFMjv93d6zZgxY/TMM89o9erVeu655xSNRjVr1izt2bOny9cpKyuTz+eLHSUlJYlUE+3OH+rTotKz9IfvfEn/99pJys/y6GBDSD97Y5eueGiD5j/2ll7b7ue+NwAASyU0TLNv3z4VFxdr48aNKi0tjZ3/3ve+p/Xr1+udd9455XOEw2GNHTtWCxcu1H333ddpmc56RkpKShimOU3hSFTrdhzQbzZV6fWP9yvSvq38OUMGac64At162WhluBOeRgQAQKf6ZJgmLy9PDodDNTU1cedrampUWFjYredwuVyaMmWKdu7c2WUZj8cjj8eTSNXQDS6HXZePK9Dl4wp0oD6oZRt36ek3dumzA436bP3neuOTg/rHy0br0jFDlOZyWF1dAECKSGiYxu12a9q0aVq7dm3sXDQa1dq1a+N6Sk4mEolo69atKioqSqym6FVDsjz67tzz9M5dl+m/Fk7R4EFufVgd0M3PbdYVD23QWzsPMnwDAEiKhPvkb7/9dl1//fW64IILNH36dD388MNqbGzUt771LUnSokWLVFxcrLKyMknSvffeq5kzZ2rUqFGqra3VAw88oN27d+vGG2/s3ZagR7Iz3PqrSUN14Vk5+tmGXfqfD/ap8nCTrnv6HU0o9umbs87S/5pUJI+TnhIAQN9IOIwsWLBABw4c0N133y2/36/Jkyfr1VdfjU1qrayslN1+tMPlyJEjuummm+T3+5WTk6Np06Zp48aNGjduXO+1AqetyJeuu68ep9suH60HXt2h5zdVaeveOt3xwl9U9spH+tZFI3XjxSMJJQCAXsd28OjU4caQfv1upZ57e7eq61okSWcNztC3LzlHX5tazJwSAMApcW8a9IrWSFS//8s+lb3ysQ7Ut61wyslw6boZI7SodITyvWztDwDoHGEEvaoh2Krn36vSs2/t0p4jbfe8cdht+vKYIfr6tGH6ynkF7OwKAIhDGEGfiESN/rjdr2fe2qX3vjgSO5+T4dI1k4v19WnDdP5QL/cfAgAQRtD3du5v0G/f36Pfvb9HNYGjm9SdV5ilr08bpvlTipWXyX4xAJCqCCNImkjU6I1PD+jFzXv0xw9rFGqNSpKcdpsuHTNEfzN1mL4yNp+VOACQYggjsERdU1j/88E+vbh5jyqqamPnfekuXTGuQJeNLdDFo/M0yMO28wAw0BFGYLlPa+r1uy17tfL9vfIHWmLnPU675owr0OzRQ3TR6DwVZ6dbWEsAQF8hjOCMEYkavbPrkP704X6t/bhGuw81xT1+wYgcXT1pqC48K1djCrPksDP5FQAGAsIIzkjGGG3fF9BLH1TrvS8O6/3KIzr2G5iT4dLsc4foy+fl66JReUyABYB+jDCCfsFf16KXPtintR/t19a9dWoItsY9PqYgS7PHDNFXzsvXyLxBKmCTNQDoNwgj6HdaI1G9X1mrtR/XaP2OA/rYX39CmXOGDNL4Yp+GZqdrxshcfWlUnpwONlsDgDMRYQT93uHGkDZ+dlCvba/R+7uPyB9oUSQa/3UdnpuhqcOzNSo/U3PPL9TIvEGEEwA4QxBGMOAEWsJ645OD2lfbrE/312vNhzU60hSOK+Ny2DRleI4mFPtU4PWoODtDXz5viDLcLCUGgGQjjGDAawq1as2HNfLXtejNnQf17q7DCrZvuHasnAyXhuVkqMiXpmsvKFFeplvjhnrZhA0A+hhhBCknGjWqPNyktz47qMpDTaoJtGjT7iOxG/sdKyfDpanDc5Tmcigrzan/NXGoLho1mHvqAEAvIowAapsUW/75IbWEo9rwyQGVf35IhxtDOtwYOqGsN82pMYVZGl2QpTEFWTq3IEu+dJeCrRHZbTadP9TLfBQASABhBOhCaySqjZ8dUnVds1rCUe3c36AXN+9Rczhy0uvGFXl1zeShGuRxamh2mqYNz5Uvw5WkWgNA/0MYARIQbI3os/2N+nR/vXb46/VJTYM+qalXczgij9Ou2qbwCXug2GzSsJx0nTV4kM4aPEij8jM1Oj9TJbkZyvd6mJMCIOURRoBedKghqJ+/uUv+QIsCza36/GCDPj/Q2GV5u00qyc3QOUMyNXiQW7mZbpXkZKgkN0O+dJcGuR3KHeTWYHaYBTCAEUaAPnawIajPDzTqi0ON+vxAo3bub9DO/fXaV9eiUCerejqTn+XRuKFejcwbpOLs9LYjp+3f3EFuJtQC6NcII4BFjDE60BDUZ/sb9fnBBtU2hXWgPqg9R5q050iz6lta1RRqVW1zWCf7ry/NZdfQ7HQV+dJkt9k0YnCGSnIyVN/Sqsw0py4YkaOpw3Nkt9vUGokyuRbAGYcwApzhmkKt+qi6Xh9WB7TncJP21DZr75Fm7att1v76YLeew+2wy2G3qTkc0YRin8YWZSnd5VCGx6ni7HSV5GYoJ8Mlh92mwYM8GpzplovQAiBJCCNAPxZsjai6tkX7apvlD7SoNWr0UXVAhxtDyk536WBDSOt27Fdj6OQrgDqTk+FSXqZHeZke5Q5ya09tszwOu66aWKQiX5oy05zK8riUmeZUmsuuw40h5Q5yq9CbxrARgIQQRoABLtga0cGGkMKtUbmcdpV/dkg1gRa1hCMKNIe1t7ZZlYebVN/Sqtao0eHG0An39klEVppTRb40FXjTFIkaOR12FXo9KvC2nSv0pik7wyW73aa8QR55XHa5HXb50tvOAUg9hBEAcaJRoyNNIR1sCOlgQ7D9CCk/y6Pquma9ufOQ6lvCqm9pVUNLqxqCrWoOR5ST4dKRpnCPg4zNJvnSXfKmuZSV5lRWmlOZHpe87T9npbX1wnT8nJXmVJbHqcw0p4yRIlGjSNTIYbdpVH6m0lwsmQb6C8IIgF7TEo6o6nCT/IEW7Q8E5XTYFGyNqqauRf5Ai2oCLaoJBBVoCas1YnSoMajWiFHrafTEdMZpt8mX7lKGx6EMl1Ppbocy2o90t1OD3I5jzjnb5s+0n/M4HUpz2ZXucsjjcuhIY0itUaOS3HQNz83gZopAH+ju32/+6wNwSmkuh0YXtG2Vn4hQa1S1zSHVNoVV3xJWoL3Xpb6l9WgvTLBVgWN6ZOqDbT83Bltls9nksNnksNvUFGrVkaawDjWGdKjrLV56zG6TXA673M624SWXw640l11prrYw425/zOO0x5Vzt/8ejkT1UXVAaS6HSnIyNCynbZm23WbTwYagWqNG00bkqGPAyu1se+4Mt0OZHqcGedp6go40hZST4ZbbyURjpA7CCIA+43balZ+VpvystNN+LmOMagJB1TaH1BSKqDkUUVMooqZQa/u/ETXH/RxRU7jtXEs4quZwRC3hiJrDEQXDUfnS21YZVR1pUm1TWFEjBVujnd75OXGHTutqm03KcDnksNvkbF8x5bTb4v51tZ/PSnNqZN4g7a1tkS/dpSGZHoUj0fawY1ea0yFPe6hy2G3atjegaNToS6PzVF3XdhPJTI9LgzxtN40c5HbK6bDFlp23vaZddnvbzw6bTXa7TW6HXa3tQ38ZboeKs9OVlRZ/ewRjjKKm7V+WnuNkGKYBkPLqW8JqDkUUikQVjhiFWqMKtUYVbG0LL02hiMKRaOx8KHLcv+0BZkxhliJRo6rDzao60qR9tc2y22zKznAp1BrV1r11sR6PUGtULeGIGkORuE3ybDaddP+ZM1mmxxmb4xMxJm6eUaE3Td70tv//PbZ96W6HfOlHQ0ysR8rpkNtpV3M4ImOkDLdD4YhRVppTwdaIQq1tQ2ytESMjI4fdLmf7ROm9R5qV5rJrVH6mXA677HabolEju82mDI+j/b1ve8/dzo5eLps+P9ioUGtUF48eokBzWOntvVatUSOP0669R5rVGjXKHeTWII9DRxrDyvd6VJCVpoZQq1wOW2w4MBqVqo40afPuIxqana6LR+XJZjsaeG02aZDbKccpJndHo6ZbE8C7Wy7ZmDMCAP1EOBJVY7BVUSNlp7t0qDGkplDbKqhI1Kg10v5vNNr+r4n9WxNoUeWhJg3NTldtc0iB5la5HTYFI1EFw22BqiV8NFwNzx2kYGtEWyprNXLIILkddjUE24bIGkNtw2bHhohYuIgaRc3Rn8MRI5tNyslwx4bQ0DW3w65wNHpC0Ex3OTTI45TbER8kPC6H6lvCOtjQ1vPkTXO19eq1RlTkS4+FpdxBbjWFItp1sFE5GS4NzU5XmtOhpnCrmoIR2e02pbscSnc5lOZ2KN1ll7+uRVVHmpWV5lRTKKJCb5pmnztE114wTCMGD+rVdjNnBAD6CZfDruwMd+z3IVkeSf3rvkW1TSEdaQrHDSfZ2/+NRI0qDzfF3RnbJpuMjJqCEQVajgaZ1ohRS2vbMFuoNap0d9vqqZZwRA67XYGWsNyOtl6QvbXNcjvtsttssXAWjRoV+NLUFGxV5eGmWIBy2O2KRKNqDLXd/LLjRpZtvVttr1XoS1M4YrTpi8Mq8Kappb1XzOVo66Ep9KYpzWXXkaa2eU2+dKeq61rUFIrI7bArFIkf4ktz2TWxOFsf+wMKtMTfaLNDc/vQ4cl0DD122HWwfdJUo7TnSHPsfNtKuVA3Pq02hxvbyh6oD2rr3jpdNCqv18NIdxFGAACnLTvDHReojjdQbwrZ1ksUVZrLIWOMQpGj846yPE7ZbDa1hCM6UB+Mzd1JczoUNUaNwVY1BiMn9EYZGbWEo0pz2VXkS1dzqC2wdUyW9te1KDvD1Tb5uzksl8OukXmDdKQppAP1QbWEo+0rzhyKGsVCVXP7HKrsDLfOGZKpxlCr0l0O7fDX663PDmraiByr3saeDdM89thjeuCBB+T3+zVp0iT95Cc/0fTp07ss/8ILL+gHP/iBvvjiC40ePVr//u//rq9+9avdfj2GaQAA6H+6+/c74enNzz//vG6//Xbdc889ev/99zVp0iTNnTtX+/fv77T8xo0btXDhQt1www3asmWL5s+fr/nz52vbtm2JvjQAABiAEu4ZmTFjhi688EI9+uijkqRoNKqSkhJ95zvf0Z133nlC+QULFqixsVEvvfRS7NzMmTM1efJkPfHEE916TXpGAADof/qkZyQUCmnz5s2aM2fO0Sew2zVnzhyVl5d3ek15eXlceUmaO3dul+UlKRgMKhAIxB0AAGBgSiiMHDx4UJFIRAUFBXHnCwoK5Pf7O73G7/cnVF6SysrK5PP5YkdJSUki1QQAAP3IGbkl3tKlS1VXVxc7qqqqrK4SAADoIwkt7c3Ly5PD4VBNTU3c+ZqaGhUWFnZ6TWFhYULlJcnj8cjjGZjLwAAAQLyEekbcbremTZumtWvXxs5Fo1GtXbtWpaWlnV5TWloaV16S1qxZ02V5AACQWhLe9Oz222/X9ddfrwsuuEDTp0/Xww8/rMbGRn3rW9+SJC1atEjFxcUqKyuTJN16662aPXu2HnzwQV111VVasWKFNm3apKeeeqp3WwIAAPqlhMPIggULdODAAd19993y+/2aPHmyXn311dgk1crKStntRztcZs2apeXLl+v73/++7rrrLo0ePVqrVq3S+PHje68VAACg3+JGeQAAoE/02Q6sAAAAvYkwAgAALEUYAQAAlkp4AqsVOqa1sC08AAD9R8ff7VNNT+0XYaS+vl6S2BYeAIB+qL6+Xj6fr8vH+8Vqmmg0qn379ikrK0s2m63XnjcQCKikpERVVVUpu0on1d+DVG+/xHsg8R6kevsl3oO+ar8xRvX19Ro6dGjcth/H6xc9I3a7XcOGDeuz5/d6vSn55TtWqr8Hqd5+ifdA4j1I9fZLvAd90f6T9Yh0YAIrAACwFGEEAABYKqXDiMfj0T333JPSdwhO9fcg1dsv8R5IvAep3n6J98Dq9veLCawAAGDgSumeEQAAYD3CCAAAsBRhBAAAWIowAgAALJXSYeSxxx7TWWedpbS0NM2YMUPvvvuu1VXqEz/84Q9ls9nijvPOOy/2eEtLixYvXqzBgwcrMzNTf/M3f6OamhoLa3z6NmzYoKuvvlpDhw6VzWbTqlWr4h43xujuu+9WUVGR0tPTNWfOHH366adxZQ4fPqzrrrtOXq9X2dnZuuGGG9TQ0JDEVvTcqdr/zW9+84TvxJVXXhlXpj+3X5LKysp04YUXKisrS/n5+Zo/f7527NgRV6Y73/3KykpdddVVysjIUH5+vr773e+qtbU1mU3pke60/9JLLz3he3DzzTfHlemv7Zekxx9/XBMnToxt5FVaWqpXXnkl9vhA/vylU7f/jPr8TYpasWKFcbvd5plnnjHbt283N910k8nOzjY1NTVWV63X3XPPPeb888831dXVsePAgQOxx2+++WZTUlJi1q5dazZt2mRmzpxpZs2aZWGNT9/LL79s/vVf/9X87ne/M5LMypUr4x6///77jc/nM6tWrTJ/+ctfzF/91V+ZkSNHmubm5liZK6+80kyaNMm8/fbb5o033jCjRo0yCxcuTHJLeuZU7b/++uvNlVdeGfedOHz4cFyZ/tx+Y4yZO3euefbZZ822bdtMRUWF+epXv2qGDx9uGhoaYmVO9d1vbW0148ePN3PmzDFbtmwxL7/8ssnLyzNLly61okkJ6U77Z8+ebW666aa470FdXV3s8f7cfmOM+f3vf2/+8Ic/mE8++cTs2LHD3HXXXcblcplt27YZYwb252/Mqdt/Jn3+KRtGpk+fbhYvXhz7PRKJmKFDh5qysjILa9U37rnnHjNp0qROH6utrTUul8u88MILsXMfffSRkWTKy8uTVMO+dfwf42g0agoLC80DDzwQO1dbW2s8Ho/59a9/bYwx5sMPPzSSzHvvvRcr88orrxibzWb27t2btLr3hq7CyDXXXNPlNQOp/R32799vJJn169cbY7r33X/55ZeN3W43fr8/Vubxxx83Xq/XBIPB5DbgNB3ffmPa/hjdeuutXV4zkNrfIScnxzz99NMp9/l36Gi/MWfW55+SwzShUEibN2/WnDlzYufsdrvmzJmj8vJyC2vWdz799FMNHTpUZ599tq677jpVVlZKkjZv3qxwOBz3Xpx33nkaPnz4gH0vdu3aJb/fH9dmn8+nGTNmxNpcXl6u7OxsXXDBBbEyc+bMkd1u1zvvvJP0OveFdevWKT8/X2PGjNEtt9yiQ4cOxR4biO2vq6uTJOXm5krq3ne/vLxcEyZMUEFBQazM3LlzFQgEtH379iTW/vQd3/4Ov/rVr5SXl6fx48dr6dKlampqij02kNofiUS0YsUKNTY2qrS0NOU+/+Pb3+FM+fz7xY3yetvBgwcViUTi3mBJKigo0Mcff2xRrfrOjBkztGzZMo0ZM0bV1dX60Y9+pIsvvljbtm2T3++X2+1WdnZ23DUFBQXy+/3WVLiPdbSrs8+/4zG/36/8/Py4x51Op3JzcwfE+3LllVfqa1/7mkaOHKnPPvtMd911l+bNm6fy8nI5HI4B1/5oNKrbbrtNF110kcaPHy9J3fru+/3+Tr8nHY/1F521X5L+9m//ViNGjNDQoUP1wQcf6F/+5V+0Y8cO/e53v5M0MNq/detWlZaWqqWlRZmZmVq5cqXGjRunioqKlPj8u2q/dGZ9/ikZRlLNvHnzYj9PnDhRM2bM0IgRI/Sb3/xG6enpFtYMVvnGN74R+3nChAmaOHGizjnnHK1bt06XXXaZhTXrG4sXL9a2bdv05ptvWl0VS3TV/m9/+9uxnydMmKCioiJddtll+uyzz3TOOecku5p9YsyYMaqoqFBdXZ1efPFFXX/99Vq/fr3V1Uqarto/bty4M+rzT8lhmry8PDkcjhNmTdfU1KiwsNCiWiVPdna2zj33XO3cuVOFhYUKhUKqra2NKzOQ34uOdp3s8y8sLNT+/fvjHm9tbdXhw4cH5Pty9tlnKy8vTzt37pQ0sNq/ZMkSvfTSS/rzn/+sYcOGxc5357tfWFjY6fek47H+oKv2d2bGjBmSFPc96O/td7vdGjVqlKZNm6aysjJNmjRJjzzySMp8/l21vzNWfv4pGUbcbremTZumtWvXxs5Fo1GtXbs2bixtoGpoaNBnn32moqIiTZs2TS6XK+692LFjhyorKwfsezFy5EgVFhbGtTkQCOidd96Jtbm0tFS1tbXavHlzrMzrr7+uaDQa+w92INmzZ48OHTqkoqIiSQOj/cYYLVmyRCtXrtTrr7+ukSNHxj3ene9+aWmptm7dGhfM1qxZI6/XG+vqPlOdqv2dqaiokKS470F/bX9XotGogsHggP/8u9LR/s5Y+vn36nTYfmTFihXG4/GYZcuWmQ8//NB8+9vfNtnZ2XGzhgeKO+64w6xbt87s2rXLvPXWW2bOnDkmLy/P7N+/3xjTtrxt+PDh5vXXXzebNm0ypaWlprS01OJan576+nqzZcsWs2XLFiPJ/Od//qfZsmWL2b17tzGmbWlvdna2Wb16tfnggw/MNddc0+nS3ilTpph33nnHvPnmm2b06NH9ZmnrydpfX19v/vmf/9mUl5ebXbt2mT/96U9m6tSpZvTo0aalpSX2HP25/cYYc8sttxifz2fWrVsXt3SxqakpVuZU3/2OpY1XXHGFqaioMK+++qoZMmRIv1jaear279y509x7771m06ZNZteuXWb16tXm7LPPNpdccknsOfpz+40x5s477zTr1683u3btMh988IG58847jc1mM3/84x+NMQP78zfm5O0/0z7/lA0jxhjzk5/8xAwfPty43W4zffp08/bbb1tdpT6xYMECU1RUZNxutykuLjYLFiwwO3fujD3e3Nxs/uEf/sHk5OSYjIwM89d//demurrawhqfvj//+c9G0gnH9ddfb4xpW977gx/8wBQUFBiPx2Muu+wys2PHjrjnOHTokFm4cKHJzMw0Xq/XfOtb3zL19fUWtCZxJ2t/U1OTueKKK8yQIUOMy+UyI0aMMDfddNMJQbw/t98Y02n7JZlnn302VqY73/0vvvjCzJs3z6Snp5u8vDxzxx13mHA4nOTWJO5U7a+srDSXXHKJyc3NNR6Px4waNcp897vfjdtnwpj+235jjPn7v/97M2LECON2u82QIUPMZZddFgsixgzsz9+Yk7f/TPv8bcYY07t9LQAAAN2XknNGAADAmYMwAgAALEUYAQAAliKMAAAASxFGAACApQgjAADAUoQRAABgKcIIAACwFGEEAABYijACAAAsRRgBAACWIowAAABL/T/foL48UIV5SwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Die Loss-Kurve wird visualisiert und in der Datei Plot_of_loss_values.png im PNG-Format gespeichert.\n",
    "loss_values = mlp.loss_curve_\n",
    "plt.plot(loss_values)\n",
    "# Check, if there is already a file with the same name\n",
    "# If yes, add a number to the filename\n",
    "i = 0\n",
    "while os.path.isfile(\"./output/plot_of_loss_values.png\"):\n",
    "    i += 1\n",
    "    plt.savefig(\"./output/plot_of_loss_values\" + str(i) + \".png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
